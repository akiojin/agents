# File API 兼容性分析与系统设计

## 1. 问题背景

`gemini-cli` 的核心是作为 Google Gemini API 的客户端。Gemini API 原生支持一种“一步式”文件处理流程，即在 `generateContent` 的单次请求中，可以直接内联文件数据（base64）或传递通过其 File API 上传后获得的文件对象。

然而，`open-gemini-cli` 的目标是兼容所有 OpenAI 兼容的 API。大多数这类 API 并不支持 Gemini 的原生文件处理方式，而是采用一种“两步式”流程：先将文件上传到服务器，获得一个文件 ID，然后在后续的对话请求中通过这个 ID 来引用文件。

我们的核心挑战是在 `open-gemini-cli` 的适配器层 (`GeminiToOpenAIConverter`) 中，将上游 Gemini 的调用模式无缝转换为下游各种 OpenAI 兼容 API 的模式。

---

## 2. 第一阶段探索：适配云端 API 的“方言”

我们最初的设想是通过在适配器层实现不同的策略，来抹平各个主流云厂商（MaaS）文件 API 的实现差异。

### 2.1. 调研发现：三种主流的云端文件处理模式

通过对 OpenAI、Moonshot(Kimi) 和阿里云 DashScope 的官方文档进行研究，我们发现所谓的“OpenAI 兼容文件接口”在实际工作流中存在三种截然不同的实现模式。

#### 模式一：OpenAI 标准引用模式 (Standard Reference)

- **适用厂商**: 标准 OpenAI API
- **核心逻辑**: 上传文件获 `file_id`，在对话请求中通过 `{"type": "input_file", "file_id": "..."}` 对象直接引用。
- **API 调用次数**: 2次 (上传 -> 对话)

#### 模式二：Moonshot 内容注入模式 (Content Injection)

- **适用厂商**: Moonshot (Kimi)
- **核心逻辑**: 上传文件获 `file_id`，**必须再调用接口获取文件抽取后的文本内容**，然后将该文本作为一个 `system` 消息注入到对话中。
- **API 调用次数**: 3次 (上传 -> 获取内容 -> 对话)

#### 模式三：DashScope URI 注入模式 (URI Injection)

- **适用厂商**: 阿里云 DashScope (qwen-long)
- **核心逻辑**: 上传文件获 `file_id`，将 ID 构造成 `fileid://{file_id}` 字符串，并将这个 **URI** 作为一个 `system` 消息注入到对话中。
- **API 调用次数**: 2次 (上传 -> 对话)

### 2.2. 初步结论：基于策略的适配器方案

基于以上发现，我们初步设计了一个基于策略的适配器，通过配置 `fileHandlingStrategy` 来为不同厂商选择不同的处理逻辑。这在当时看来是一个灵活且可行的方案。

---

## 3. 第二阶段探索：云端适配路线的局限性

随着我们对更多厂商进行调研，我们发现“适配云端 API”这条路线存在根本性的局限。

### 3.1. 更多厂商的现状

#### 模式四：SiliconFlow 异步批处理指令模式 (Async Batch Instruction)

- **适用厂商**: SiliconFlow (硅基流动)
- **核心逻辑**: 文件 API 的 `purpose` 只有 `batch` 一个选项。其文件功能**完全服务于异步批处理任务**，上传的文件（`.jsonl`）是 API 请求指令的集合，而非用于实时对话的上下文数据。
- **结论**: 与我们的交互式文档分析场景**根本不兼容**。

#### 模式五：火山引擎 - 知识库模式 (Knowledge Base)

- **适用厂商**: 火山引擎 (VolcEngine)
- **核心逻辑**: **完全没有**提供与 OpenAI 兼容的文件上传接口用于实时对话。官方推荐的路径是让用户预先将其文档上传到他们的“知识库”系统中进行管理和检索。
- **结论**: 无法通过适配器进行兼容。

#### 模式六：本地推理服务 - 不支持 (Unsupported)

- **适用厂商**: Ollama, LM Studio 等消费级本地推理框架。
- **核心逻辑**: 这些框架通常只提供核心的推理 API (`/v1/chat/completions`)，**完全不支持**文件上传或任何形式的二进制数据处理。
- **结论**: 无法通过适配器进行兼容。

### 3.2. 云端适配路线的根本挑战

我们的进一步研究揭示了一个残酷的现实：**OpenAI 文件 API 的生态是高度碎片化的，甚至在很多主流服务中是缺失的。**

试图在适配器层抹平所有差异的路线，将面临：
1.  **无限的复杂性**: 每出现一种新的“方言”，适配器就得增加一套新的复杂逻辑。
2.  **不可维护性**: 适配器将变成一个巨大的 `switch-case` 怪物，牵一发而动全身。
3.  **永远的追赶**: 我们将永远跟在厂商后面，被动地适配他们的最新实现。
4.  **功能缺失的硬伤**: 对于火山引擎和本地服务这类根本没有对应功能的厂商，适配器路线无能为力。

---

## 4. 最终方案：转向本地文件解析 (Client-Side Parsing)

鉴于云端适配路线的局限性，我们决定进行战略转向，将文件处理能力从“依赖云端”转为“本地实现”。

### 4.1. 核心思想

与其依赖并适配各个厂商不统一、不可靠的文件处理能力，不如在 `open-gemini-cli` 的客户端（例如一个新建的 `file-util` 模块）中，**自己实现一个统一、可靠的文件解析和文本提取功能**。

### 4.2. 新工作流程

1.  **用户请求**: 用户通过 `gemini` 命令提供一个或多个本地文件路径。
2.  **本地解析**: `open-gemini-cli` 内部的 `LocalFileParser` 模块被激活。它根据文件扩展名（`.pdf`, `.docx`, `.txt` 等）选择合适的本地解析库，直接在用户本地读取文件并提取为纯文本。
3.  **内容注入**: 提取出的文本被构造成一个或多个 `system` 或 `user` 消息。
4.  **发送请求**: 这个“纯文本”的、不包含任何文件引��的 `chat/completions` 请求，被发送到任何一个 OpenAI 兼容的 API 端点。

### 4.3. 新方案的优势

1.  **终极兼容性**: 绕过了所有厂商的文件 API 差异。只要模型能接受长文本，就能处理文件。火山引擎、Ollama 等瞬间被纳入支持范围。
2.  **消除碎片化**: 从根本上解决了生态碎片化问题。
3.  **稳定与可控**: 核心逻辑由我们自己掌握，不再受制于云厂商。
4.  **简化适配器**: `GeminiToOpenAIConverter` 可以回归其核心职责，逻辑更清晰。
5.  **隐私性**: 原始文件永不离开用户本地。

### 4.4. 新方案的挑战

1.  **Token 限制**: 这是最主要的挑战。我们需要一个健壮的机制来估算 Token 数量，并在内容超出目标模型上下文窗口时进行智能截断或对用户发出警告。
2.  **依赖管理**: 需要引入第三方库进行文件解析，会增加项目的依赖和最终打包体积。
3.  **解析质量**: 本地解析库对复杂文档（如图表、扫描件）的处理能力可能不如某些厂商的专用云端服务。我们需要选择高质量的库，并接受这是一种权衡。

## 5. 结论：一个战略性的飞跃

尽管面临挑战，但“本地文件解析”方案将 `open-gemini-cli` 从一个被动的“生态缝合者”转变为一个主动的“通用能力提供者”。它通过将核心能力内置，换来了无与伦比的兼容性、稳定性和可维护性，是项目走向成熟和独立的、正确的战略方向。