/**
 * @license
 * Copyright 2025 Google LLC
 * SPDX-License-Identifier: Apache-2.0
 */

import {
  EmbedContentParameters,
  GenerateContentConfig,
  Part,
  SchemaUnion,
  PartListUnion,
  Content,
  Tool,
  GenerateContentResponse,
} from '@google/genai';
import { getFolderStructure } from '../utils/getFolderStructure.js';
import {
  Turn,
  ServerGeminiStreamEvent,
  GeminiEventType,
  ChatCompressionInfo,
} from './turn.js';
import { Config } from '../config/config.js';
import { getCoreSystemPrompt, getCompressionPrompt, getDeepAgentSystemPrompt } from './prompts.js';
import { ReadManyFilesTool } from '../tools/read-many-files.js';
import { getResponseText } from '../utils/generateContentResponseUtilities.js';
import { checkNextSpeaker } from '../utils/nextSpeakerChecker.js';
import { reportError } from '../utils/errorReporting.js';
import { GeminiChat } from './geminiChat.js';
import { retryWithBackoff } from '../utils/retry.js';
import { getErrorMessage } from '../utils/errors.js';
import { isFunctionResponse } from '../utils/messageInspectors.js';
import { tokenLimit } from './tokenLimits.js';
import {
  AuthType,
  ContentGenerator,
  ContentGeneratorConfig,
  createContentGenerator,
} from './contentGenerator.js';
import { ProxyAgent, setGlobalDispatcher } from 'undici';
import { DEFAULT_AGENTS_FLASH_MODEL } from '../config/models.js';
import { LoopDetectionService } from '../services/loopDetectionService.js';
import { FileParserService, VLMService } from '../services/fileParserService.js';
import { CompositeVLMService } from '../services/vlmService.js';
import { getSessionManager } from '../utils/session-manager.js';
import { uiTelemetryService } from '../telemetry/uiTelemetry.js';
import * as path from 'path';

function isThinkingSupported(model: string) {
  if (model.startsWith('gemini-2.5')) return true;
  return false;
}

/**
 * Returns the index of the content after the fraction of the total characters in the history.
 *
 * Exported for testing purposes.
 */
export function findIndexAfterFraction(
  history: Content[],
  fraction: number,
): number {
  if (fraction <= 0 || fraction >= 1) {
    throw new Error('Fraction must be between 0 and 1');
  }

  const contentLengths = history.map(
    (content) => JSON.stringify(content).length,
  );

  const totalCharacters = contentLengths.reduce(
    (sum, length) => sum + length,
    0,
  );
  const targetCharacters = totalCharacters * fraction;

  let charactersSoFar = 0;
  for (let i = 0; i < contentLengths.length; i++) {
    charactersSoFar += contentLengths[i];
    if (charactersSoFar >= targetCharacters) {
      return i;
    }
  }
  return contentLengths.length;
}

export class GeminiClient {
  private contentGenerator?: ContentGenerator;
  private chat?: GeminiChat;
  private sessionTurnCount = 0;
  /**
   * This is the default token limit proportion used for all prompts.
   * Individual tools can override this on a per-tool basis with the
   * {@link SafetyConfig} parameter.
   */
  private readonly DEFAULT_TOKEN_LIMIT_PROPORTION = 0.8;
  private readonly generateContentConfig: GenerateContentConfig = {};
  private readonly embeddingModel: string = 'text-embedding-004';
  private readonly MAX_TURNS = 100;
  /**
   * Threshold for compression token count as a fraction of the model's token limit.
   * If the chat history exceeds this threshold, it will be compressed.
   * ÈÄöÂ∏∏„ÅØ95%„Å†„Åå„ÄÅ„ÉÑ„Éº„É´ÂÆüË°åÂæå„ÅØ85%„Å´‰∏ã„Åí„Å¶‰ΩôË£ï„ÇíÊåÅ„Åü„Åõ„Çã
   */
  private readonly COMPRESSION_TOKEN_THRESHOLD = 0.95;
  private readonly COMPRESSION_TOKEN_THRESHOLD_AFTER_TOOLS = 0.85;
  /**
   * The fraction of the latest chat history to keep. A value of 0.3
   * means that only the last 30% of the chat history will be kept after compression.
   */
  private readonly COMPRESSION_PRESERVE_THRESHOLD = 0.3;

  private readonly loopDetector: LoopDetectionService;
  private fileParserService: FileParserService;
  private vlmService?: VLMService;
  private lastPromptId?: string;

  constructor(private config: Config) {
    if (config.getProxy()) {
      setGlobalDispatcher(new ProxyAgent(config.getProxy() as string));
    }

    this.embeddingModel = config.getEmbeddingModel();
    this.loopDetector = new LoopDetectionService(config);
    
    // Initialize file parser service without VLM (will be set later)
    this.fileParserService = new FileParserService();
  }

  async initialize(contentGeneratorConfig: ContentGeneratorConfig) {
    this.contentGenerator = await createContentGenerator(
      contentGeneratorConfig,
      this.config,
      this.config.getSessionId(),
    );
    
    // Now initialize VLM service with the content generator
    this.vlmService = new CompositeVLMService(this.contentGenerator);
    this.fileParserService = new FileParserService(this.vlmService);
    
    // Memory API„ÅÆÂàùÊúüÂåñÔºàÂà©Áî®ÂèØËÉΩ„Å™Â†¥ÂêàÔºâ
    try {
      const memoryModule = await import('@agents/memory');
      const memoryAPI = memoryModule.getMemoryAPI();
      await memoryAPI.initialize();
      // Memory APIÂàùÊúüÂåñÊàêÂäü
    } catch (error) {
      // Memory API„ÅåÂà©Áî®„Åß„Åç„Å™„ÅÑÂ†¥Âêà„ÅØÈùô„Åã„Å´ÁÑ°Ë¶ñ
      console.debug('Memory API not available or failed to initialize:', error);
    }
    
    this.chat = await this.startChat();
  }

  getContentGenerator(): ContentGenerator {
    if (!this.contentGenerator) {
      throw new Error('Content generator not initialized');
    }
    return this.contentGenerator;
  }

  async addHistory(content: Content) {
    this.getChat().addHistory(content);
  }

  getChat(): GeminiChat {
    if (!this.chat) {
      throw new Error('Chat not initialized');
    }
    return this.chat;
  }

  isInitialized(): boolean {
    return this.chat !== undefined && this.contentGenerator !== undefined;
  }

  getHistory(): Content[] {
    return this.getChat().getHistory();
  }

  setHistory(history: Content[]) {
    this.getChat().setHistory(history);
  }

  async resetChat(): Promise<void> {
    this.chat = await this.startChat();
  }

  private async getEnvironment(): Promise<Part[]> {
    const cwd = this.config.getWorkingDir();
    const today = new Date().toLocaleDateString(undefined, {
      weekday: 'long',
      year: 'numeric',
      month: 'long',
      day: 'numeric',
    });
    const platform = process.platform;
    const folderStructure = await getFolderStructure(cwd, {
      fileService: this.config.getFileService(),
    });
    const context = `
  This is the Agents CLI. We are setting up the context for our chat.
  Today's date is ${today}.
  My operating system is: ${platform}
  I'm currently working in the directory: ${cwd}
  ${folderStructure}
          `.trim();

    const initialParts: Part[] = [{ text: context }];
    const toolRegistry = await this.config.getToolRegistry();

    // Add full file context if the flag is set
    if (this.config.getFullContext()) {
      try {
        const readManyFilesTool = toolRegistry.getTool(
          'read_many_files',
        ) as ReadManyFilesTool;
        if (readManyFilesTool) {
          // Read all files in the target directory
          const result = await readManyFilesTool.execute(
            {
              paths: ['**/*'], // Read everything recursively
              useDefaultExcludes: true, // Use default excludes
            },
            AbortSignal.timeout(30000),
          );
          if (result.llmContent) {
            initialParts.push({
              text: `\n--- Full File Context ---\n${result.llmContent}`,
            });
          } else {
            console.warn(
              'Full context requested, but read_many_files returned no content.',
            );
          }
        } else {
          console.warn(
            'Full context requested, but read_many_files tool not found.',
          );
        }
      } catch (error) {
        // Not using reportError here as it's a startup/config phase, not a chat/generation phase error.
        console.error('Error reading full file context:', error);
        initialParts.push({
          text: '\n--- Error reading full file context ---',
        });
      }
    }

    return initialParts;
  }

  private async startChat(extraHistory?: Content[]): Promise<GeminiChat> {
    const envParts = await this.getEnvironment();
    const toolRegistry = await this.config.getToolRegistry();
    const toolDeclarations = toolRegistry.getFunctionDeclarations();
    const tools: Tool[] = [{ functionDeclarations: toolDeclarations }];
    const history: Content[] = [
      {
        role: 'user',
        parts: envParts,
      },
      {
        role: 'model',
        parts: [{ text: 'Got it. Thanks for the context!' }],
      },
      ...(extraHistory ?? []),
    ];
    try {
      const userMemory = this.config.getUserMemory();
      
      // Ë®òÊÜ∂„Ç∑„Çπ„ÉÜ„É†„ÅÆÁµ±Ë®àÊÉÖÂ†±„ÇíÂèñÂæó
      let memoryStats = null;
      try {
        // MemoryManager„ÅåÂà©Áî®ÂèØËÉΩ„Å™Â†¥Âêà„ÅÆ„ÅøÁµ±Ë®àÊÉÖÂ†±„ÇíÂèñÂæó
        const memoryModule = await import('@agents/memory');
        const memoryAPI = memoryModule.getMemoryAPI();
        if (memoryAPI && typeof memoryAPI.getStatistics === 'function') {
          const stats = await memoryAPI.getStatistics();
          memoryStats = stats;
        }
      } catch (error) {
        // Ë®òÊÜ∂„Ç∑„Çπ„ÉÜ„É†„ÅåÂà©Áî®„Åß„Åç„Å™„ÅÑÂ†¥Âêà„ÅØÁÑ°Ë¶ñ
        console.debug('Memory statistics not available:', error);
      }
      
      // DeepAgent„Éó„É≠„É≥„Éó„Éà„Çí‰ΩøÁî®
      const systemInstruction = getDeepAgentSystemPrompt(userMemory, memoryStats);
      const generateContentConfigWithThinking = isThinkingSupported(
        this.config.getModel(),
      )
        ? {
            ...this.generateContentConfig,
            thinkingConfig: {
              includeThoughts: true,
            },
          }
        : this.generateContentConfig;
      return new GeminiChat(
        this.config,
        this.getContentGenerator(),
        {
          systemInstruction,
          ...generateContentConfigWithThinking,
          tools,
        },
        history,
      );
    } catch (error) {
      await reportError(
        error,
        'Error initializing Gemini chat session.',
        { historyLength: history?.length || 0 },
        'startChat',
      );
      throw new Error(`Failed to initialize chat: ${getErrorMessage(error)}`);
    }
  }

  private async parseFilesFromContent(
    contents: Content[],
  ): Promise<Content[]> {
    const newContents: Content[] = [];
    for (const content of contents) {
      const newParts: Part[] = [];
      if (content.parts) {
        for (const part of content.parts) {
          if (
            'functionCall' in part &&
            part.functionCall &&
            part.functionCall.name === 'file_parser' &&
            part.functionCall.args &&
            typeof part.functionCall.args.path === 'string'
          ) {
            try {
              const filePath = part.functionCall.args.path;
              const markdown =
                await this.fileParserService.parseFileToMarkdown(filePath);
              const fileName = path.basename(filePath);
              newParts.push({
                text: `## Content from file: ${fileName}\n\n${markdown}`,
              });
            } catch (e) {
              newParts.push({
                text: `Error parsing file: ${getErrorMessage(e)}`,
              });
            }
          } else {
            newParts.push(part);
          }
        }
      }
      newContents.push({ ...content, parts: newParts });
    }
    return newContents;
  }

  async *sendMessageStream(
    request: PartListUnion,
    signal: AbortSignal,
    prompt_id: string,
    turns: number = this.MAX_TURNS,
    originalModel?: string,
  ): AsyncGenerator<ServerGeminiStreamEvent, Turn> {
    if (this.lastPromptId !== prompt_id) {
      this.loopDetector.reset();
      this.lastPromptId = prompt_id;
    }
    this.sessionTurnCount++;
    if (
      this.config.getMaxSessionTurns() > 0 &&
      this.sessionTurnCount > this.config.getMaxSessionTurns()
    ) {
      yield { type: GeminiEventType.MaxSessionTurns };
      return new Turn(this.getChat(), prompt_id);
    }
    // Ensure turns never exceeds MAX_TURNS to prevent infinite loops
    const boundedTurns = Math.min(turns, this.MAX_TURNS);
    if (!boundedTurns) {
      return new Turn(this.getChat(), prompt_id);
    }

    // Track the original model from the first call to detect model switching
    const initialModel = originalModel || this.config.getModel();

    const compressed = await this.tryCompressChat(prompt_id);

    if (compressed) {
      yield { type: GeminiEventType.ChatCompressed, value: compressed };
    }

    // Pre-process files before sending the message
    const requestParts = Array.isArray(request) ? request : [request];
    const processedRequest = await this.parseFilesFromContent([
      { role: 'user', parts: requestParts.map(part => typeof part === 'string' ? { text: part } : part) },
    ]);
    const finalRequest = processedRequest[0].parts;

    const turn = new Turn(this.getChat(), prompt_id);
    const resultStream = turn.run(finalRequest || [], signal);
    for await (const event of resultStream) {
      if (this.loopDetector.addAndCheck(event)) {
        yield { type: GeminiEventType.LoopDetected };
        return turn;
      }
      yield event;
    }
    
    // „ÉÑ„Éº„É´ÂÆüË°åÂæå„Å´ÂúßÁ∏Æ„ÉÅ„Çß„ÉÉ„ÇØÔºà„Éà„Éº„ÇØ„É≥„ÅåÂ¢ó„Åà„Å¶„ÅÑ„ÇãÂèØËÉΩÊÄß„Åå„ÅÇ„Çã„Åü„ÇÅÔºâ
    // „ÉÑ„Éº„É´ÂÆüË°åÂæå„ÅØÈñæÂÄ§„Çí85%„Å´‰∏ã„Åí„Å¶„ÄÅÊ¨°„ÅÆ„Çø„Éº„É≥„Åß‰ΩôË£ï„ÇíÊåÅ„Åü„Åõ„Çã
    if (turn.pendingToolCalls.length > 0) {
      const compressedAfterTools = await this.tryCompressChat(prompt_id, false, true);
      if (compressedAfterTools) {
        yield { type: GeminiEventType.ChatCompressed, value: compressedAfterTools };
      }
    }
    
    if (!turn.pendingToolCalls.length && signal && !signal.aborted) {
      // Check if model was switched during the call (likely due to quota error)
      const currentModel = this.config.getModel();
      if (currentModel !== initialModel) {
        // Model was switched (likely due to quota error fallback)
        // Don't continue with recursive call to prevent unwanted Flash execution
        return turn;
      }

      const nextSpeakerCheck = await checkNextSpeaker(
        this.getChat(),
        this,
        signal,
      );
      if (nextSpeakerCheck?.next_speaker === 'model') {
        // ÂÜçÂ∏∞Âëº„Å≥Âá∫„ÅóÂâç„Å´ÂúßÁ∏Æ„ÉÅ„Çß„ÉÉ„ÇØÔºàÊ¨°„ÅÆ„Çø„Éº„É≥„Åß„Ç®„É©„Éº„Å´„Å™„Çâ„Å™„ÅÑ„Çà„ÅÜ„Å´Ôºâ
        const compressedBeforeRecursion = await this.tryCompressChat(prompt_id);
        if (compressedBeforeRecursion) {
          yield { type: GeminiEventType.ChatCompressed, value: compressedBeforeRecursion };
        }
        
        const nextRequest = [{ text: 'Please continue.' }];
        // This recursive call's events will be yielded out, but the final
        // turn object will be from the top-level call.
        yield* this.sendMessageStream(
          nextRequest,
          signal,
          prompt_id,
          boundedTurns - 1,
          initialModel,
        );
      }
    }
    return turn;
  }

  async generateJson(
    contents: Content[],
    schema: SchemaUnion,
    abortSignal: AbortSignal,
    model?: string,
    config: GenerateContentConfig = {},
  ): Promise<Record<string, unknown>> {
    // Use current model from config instead of hardcoded Flash model
    const modelToUse =
      model || this.config.getModel() || DEFAULT_AGENTS_FLASH_MODEL;
    try {
      const userMemory = this.config.getUserMemory();
      const systemInstruction = getCoreSystemPrompt(userMemory);
      const requestConfig = {
        abortSignal,
        ...this.generateContentConfig,
        ...config,
      };

      const processedContents = await this.parseFilesFromContent(contents);

      const apiCall = () =>
        this.getContentGenerator().generateContent({
          model: modelToUse,
          config: {
            ...requestConfig,
            systemInstruction,
            responseSchema: schema,
            responseMimeType: 'application/json',
          },
          contents: processedContents,
        });

      const result = await retryWithBackoff(apiCall, {
        authType: this.config.getContentGeneratorConfig()?.authType,
      });

      const text = getResponseText(result);
      if (!text) {
        const error = new Error(
          'API returned an empty response for generateJson.',
        );
        await reportError(
          error,
          'Error in generateJson: API returned an empty response.',
          { contentsLength: contents?.length || 0 },
          'generateJson-empty-response',
        );
        throw error;
      }
      try {
        return JSON.parse(text);
      } catch (parseError) {
        await reportError(
          parseError,
          'Failed to parse JSON response from generateJson.',
          {
            responseTextLength: text?.length || 0,
            responseTextPreview: text?.substring(0, 200),
            contentsLength: contents?.length || 0,
          },
          'generateJson-parse',
        );
        throw new Error(
          `Failed to parse API response as JSON: ${getErrorMessage(parseError)}`,
        );
      }
    } catch (error) {
      if (abortSignal.aborted) {
        throw error;
      }

      // Avoid double reporting for the empty response case handled above
      if (
        error instanceof Error &&
        error.message === 'API returned an empty response for generateJson.'
      ) {
        throw error;
      }

      await reportError(
        error,
        'Error generating JSON content via API.',
        { contentsLength: contents?.length || 0 },
        'generateJson-api',
      );
      throw new Error(
        `Failed to generate JSON content: ${getErrorMessage(error)}`,
      );
    }
  }

  async generateContent(
    contents: Content[],
    generationConfig: GenerateContentConfig,
    abortSignal: AbortSignal,
    model?: string,
  ): Promise<GenerateContentResponse> {
    const modelToUse = model ?? this.config.getModel();
    const configToUse: GenerateContentConfig = {
      ...this.generateContentConfig,
      ...generationConfig,
    };

    try {
      const userMemory = this.config.getUserMemory();
      const systemInstruction = getCoreSystemPrompt(userMemory);

      const requestConfig = {
        abortSignal,
        ...configToUse,
        systemInstruction,
      };

      const processedContents = await this.parseFilesFromContent(contents);

      const apiCall = () =>
        this.getContentGenerator().generateContent({
          model: modelToUse,
          config: requestConfig,
          contents: processedContents,
        });

      const result = await retryWithBackoff(apiCall, {
        authType: this.config.getContentGeneratorConfig()?.authType,
      });
      return result;
    } catch (error: unknown) {
      if (abortSignal.aborted) {
        throw error;
      }

      await reportError(
        error,
        `Error generating content via API with model ${modelToUse}.`,
        {
          contentsLength: contents?.length || 0,
          modelUsed: modelToUse,
        },
        'generateContent-api',
      );
      throw new Error(
        `Failed to generate content with model ${modelToUse}: ${getErrorMessage(error)}`,
      );
    }
  }

  async generateEmbedding(texts: string[]): Promise<number[][]> {
    if (!texts || texts.length === 0) {
      return [];
    }
    const embedModelParams: EmbedContentParameters = {
      model: this.embeddingModel,
      contents: texts,
    };

    const embedContentResponse =
      await this.getContentGenerator().embedContent(embedModelParams);
    if (
      !embedContentResponse.embeddings ||
      embedContentResponse.embeddings.length === 0
    ) {
      throw new Error('No embeddings found in API response.');
    }

    if (embedContentResponse.embeddings.length !== texts.length) {
      throw new Error(
        `API returned a mismatched number of embeddings. Expected ${texts.length}, got ${embedContentResponse.embeddings.length}.`,
      );
    }

    return embedContentResponse.embeddings.map((embedding, index) => {
      const values = embedding.values;
      if (!values || values.length === 0) {
        throw new Error(
          `API returned an empty embedding for input text at index ${index}: "${texts[index]}"`,
        );
      }
      return values;
    });
  }

  async tryCompressChat(
    prompt_id: string,
    force: boolean = false,
    useToolThreshold: boolean = false,
  ): Promise<ChatCompressionInfo | null> {
    const curatedHistory = this.getChat().getHistory(true);

    // Regardless of `force`, don't do anything if the history is empty.
    if (curatedHistory.length === 0) {
      return null;
    }

    const model = this.config.getModel();

    // UI„Å®‰∏ÄËá¥„Åï„Åõ„Çã„Åü„ÇÅ„ÄÅÁ¥ØÁ©ç„Éà„Éº„ÇØ„É≥Êï∞ÔºàÂÖ®„É¢„Éá„É´ÂêàË®àÔºâ„ÇíÂèñÂæó
    const metrics = uiTelemetryService.getMetrics();
    const cumulativeTokenCount = Object.values(metrics.models).reduce(
      (total, modelMetrics) => total + modelMetrics.tokens.prompt,
      0
    );

    // ÁèæÂú®„ÅÆ„É¢„Éá„É´„ÅÆÂçò‰Ωì„Éà„Éº„ÇØ„É≥Êï∞„ÇÇÂèñÂæóÔºàÂèÇËÄÉÁî®Ôºâ
    const { totalTokens: currentModelTokenCount } =
      await this.getContentGenerator().countTokens({
        model,
        contents: curatedHistory,
      });
    if (currentModelTokenCount === undefined) {
      console.warn(`Could not determine token count for model ${model}.`);
      return null;
    }

    const modelTokenLimit = tokenLimit(model);
    // ÂúßÁ∏ÆÂà§ÂÆö„Å´„ÅØÁ¥ØÁ©ç„Éà„Éº„ÇØ„É≥Êï∞„Çí‰ΩøÁî®ÔºàUI„ÅÆË°®Á§∫„Å®‰∏ÄËá¥Ôºâ
    const originalTokenCount = cumulativeTokenCount || currentModelTokenCount;
    // „ÉÑ„Éº„É´ÂÆüË°åÂæå„ÅØÈñæÂÄ§„Çí‰∏ã„Åí„Å¶‰ΩôË£ï„ÇíÊåÅ„Åü„Åõ„Çã
    const thresholdRatio = useToolThreshold 
      ? this.COMPRESSION_TOKEN_THRESHOLD_AFTER_TOOLS 
      : this.COMPRESSION_TOKEN_THRESHOLD;
    const threshold = thresholdRatio * modelTokenLimit;
    const remainingTokens = modelTokenLimit - originalTokenCount;
    const usagePercentage = Math.round(originalTokenCount / modelTokenLimit * 100);
    
    // „Éá„Éê„ÉÉ„Ç∞„É≠„Ç∞„ÇíËøΩÂä†Ôºà„Çà„ÇäË©≥Á¥∞„Å´Ôºâ
    console.log(`[Compression Debug] Model: ${model}, Token Limit: ${modelTokenLimit}`);
    console.log(`[Compression Debug] Current Usage: ${originalTokenCount}/${modelTokenLimit} (${usagePercentage}%)`);
    console.log(`[Compression Debug] Remaining: ${remainingTokens} tokens`);
    console.log(`[Compression Debug] Compression Threshold: ${threshold} (${Math.round(thresholdRatio * 100)}%)`);
    console.log(`[Compression Debug] Force flag: ${force}, Tool threshold: ${useToolThreshold}`);
    
    // UI„Å´Ë°®Á§∫„Åï„Çå„ÇãÊÆãÈáè„Åå0%„ÅÆÂ†¥Âêà„ÅÆË≠¶Âëä
    if (usagePercentage >= 100 && !force) {
      console.warn(`[Compression Warning] Context is at ${usagePercentage}% but compression threshold is ${Math.round(thresholdRatio * 100)}%`);
      console.warn(`[Compression Warning] UI may show 0% context remaining. Use /compress command to manually compress.`);
    }
    
    // ÂúßÁ∏ÆÊù°‰ª∂„ÇíÂà§ÂÆö
    const shouldCompress = force || originalTokenCount >= threshold;
    
    if (!shouldCompress) {
      console.log(`[Compression Debug] Skipping compression: usage ${usagePercentage}% is below threshold ${Math.round(this.COMPRESSION_TOKEN_THRESHOLD * 100)}%`);
      return null;
    }

    console.log(`[Compression Debug] Starting compression...`);
    console.log(`\n‚è≥ ÂúßÁ∏ÆÂá¶ÁêÜ„ÇíÈñãÂßã„Åó„Å¶„ÅÑ„Åæ„Åô...`);
    console.log(`üìä „Çπ„ÉÜ„ÉÉ„Éó1/4: „Éà„Éº„ÇØ„É≥Êï∞„ÇíË®àÁÆó‰∏≠...`);

    let compressBeforeIndex = findIndexAfterFraction(
      curatedHistory,
      1 - this.COMPRESSION_PRESERVE_THRESHOLD,
    );
    // Find the first user message after the index. This is the start of the next turn.
    while (
      compressBeforeIndex < curatedHistory.length &&
      (curatedHistory[compressBeforeIndex]?.role === 'model' ||
        isFunctionResponse(curatedHistory[compressBeforeIndex]))
    ) {
      compressBeforeIndex++;
    }

    const historyToCompress = curatedHistory.slice(0, compressBeforeIndex);
    const historyToKeep = curatedHistory.slice(compressBeforeIndex);

    console.log(`üìù „Çπ„ÉÜ„ÉÉ„Éó2/4: Â±•Ê≠¥„ÇíÂàÜÊûê‰∏≠... (ÂúßÁ∏ÆÂØæË±°: ${historyToCompress.length}‰ª∂, ‰øùÊåÅ: ${historyToKeep.length}‰ª∂)`);

    // „Çª„ÉÉ„Ç∑„Éß„É≥„Éû„Éç„Éº„Ç∏„É£„Éº„ÇíÂèñÂæó
    const sessionManager = getSessionManager();
    
    // ÂúßÁ∏ÆÂâç„ÅÆÂ±•Ê≠¥„Çí„Çª„ÉÉ„Ç∑„Éß„É≥„Å´‰øùÂ≠ò
    sessionManager.updateHistory(curatedHistory);
    sessionManager.updateTokenCount(originalTokenCount);

    // Memory API„Å´ÂúßÁ∏ÆÂâç„ÅÆÈáçË¶ÅÊÉÖÂ†±„Çí‰øùÂ≠òÔºàÂà©Áî®ÂèØËÉΩ„Å™Â†¥ÂêàÔºâ
    try {
      const memoryModule = await import('@agents/memory');
      const memoryAPI = memoryModule.getMemoryAPI();
      
      // ÂúßÁ∏Æ„Ç§„Éô„É≥„Éà„Çí„É°„É¢„É™„Å´Ë®òÈå≤
      await memoryAPI.recordEvent({
        type: 'discovery',
        content: {
          event: 'compression_start',
          sessionId: sessionManager.getCurrentSessionId(),
          originalTokenCount,
          historyLength: curatedHistory.length,
          compressBeforeIndex,
          preservedMessages: historyToKeep.length,
        },
        context: {
          model: model,
          threshold: this.COMPRESSION_TOKEN_THRESHOLD,
        },
        timestamp: new Date(),
      });
      
      console.log('[Compression] Saved compression event to Memory API');
    } catch (error) {
      // Memory API„ÅåÂà©Áî®„Åß„Åç„Å™„ÅÑÂ†¥Âêà„ÅØÈùô„Åã„Å´ÁÑ°Ë¶ñ
      console.debug('[Compression] Memory API not available:', error);
    }

    this.getChat().setHistory(historyToCompress);

    console.log(`ü§ñ „Çπ„ÉÜ„ÉÉ„Éó3/4: „Çµ„Éû„É™„Éº„ÇíÁîüÊàê‰∏≠...`);
    const { text: summary } = await this.getChat().sendMessage(
      {
        message: {
          text: 'First, reason in your scratchpad. Then, generate the <state_snapshot>.',
        },
        config: {
          systemInstruction: { text: getCompressionPrompt() },
        },
      },
      prompt_id,
    );
    
    // ÂúßÁ∏ÆÂæå„ÅÆÊñ∞„Åó„ÅÑÂ±•Ê≠¥„Çí‰ΩúÊàê
    const compressedHistory = [
      {
        role: 'user' as const,
        parts: [{ text: summary }],
      },
      {
        role: 'model' as const,
        parts: [{ text: 'Got it. Thanks for the additional context!' }],
      },
      ...historyToKeep,
    ];

    console.log(`üíæ „Çπ„ÉÜ„ÉÉ„Éó4/4: Êñ∞„Åó„ÅÑ„Çª„ÉÉ„Ç∑„Éß„É≥„Çí‰ΩúÊàê‰∏≠...`);

    // ÂúßÁ∏ÆÂæå„ÅÆ„Çª„ÉÉ„Ç∑„Éß„É≥„Çí‰ΩúÊàê
    const newSession = await sessionManager.compressAndStartNewSession(
      compressedHistory,
      summary || '',  // summary„Ååundefined„ÅÆÂ†¥Âêà„ÅØÁ©∫ÊñáÂ≠óÂàó
      originalTokenCount,
      0  // newTokenCount„ÅØÂæå„ÅßË®àÁÆó„Åï„Çå„Çã„Åü„ÇÅ‰∏ÄÊôÇÁöÑ„Å´0
    );

    // Memory API„Å´ÂúßÁ∏Æ„Çµ„Éû„É™„Éº„Çí‰øùÂ≠òÔºàÂà©Áî®ÂèØËÉΩ„Å™Â†¥ÂêàÔºâ
    try {
      const memoryModule = await import('@agents/memory');
      const memoryAPI = memoryModule.getMemoryAPI();
      
      // ÂúßÁ∏Æ„Çµ„Éû„É™„Éº„Çí„É°„É¢„É™„Å´‰øùÂ≠ò
      await memoryAPI.recordProjectInfo(
        `compression_summary_${newSession.id}`,
        {
          summary: summary,
          sessionId: newSession.id,
          parentSessionId: newSession.parentSessionId,
          compressedFrom: originalTokenCount,
          timestamp: new Date().toISOString(),
        }
      );
      
      console.log('[Compression] Saved compression summary to Memory API');
    } catch (error) {
      // Memory API„ÅåÂà©Áî®„Åß„Åç„Å™„ÅÑÂ†¥Âêà„ÅØÈùô„Åã„Å´ÁÑ°Ë¶ñ
      console.debug('[Compression] Memory API summary save failed:', error);
    }

    // Reinitialize the chat with the new history.
    this.chat = await this.startChat(compressedHistory);

    const { totalTokens: newTokenCount } =
      await this.getContentGenerator().countTokens({
        model,
        contents: this.getChat().getHistory(true),
      });
    
    // ÂúßÁ∏ÆÂæå„ÅÆ„Éà„Éº„ÇØ„É≥Êï∞„ÇíÊõ¥Êñ∞
    sessionManager.updateTokenCount(newTokenCount ?? 0);
    await sessionManager.saveSession();
    
    // „ÉÜ„É¨„É°„Éà„É™„Çµ„Éº„Éì„Çπ„ÅÆ„Éà„Éº„ÇØ„É≥„Ç´„Ç¶„É≥„Éà„Çí„É™„Çª„ÉÉ„ÉàÔºàUI„ÅÆË°®Á§∫„ÇíÊõ¥Êñ∞Ôºâ
    uiTelemetryService.resetTokenCountAfterCompression(newTokenCount ?? 0, model);
    
    console.log(`[Compression Debug] Compression complete: ${originalTokenCount} -> ${newTokenCount} tokens`);
    console.log(`[Compression Debug] Reduction: ${Math.round((1 - (newTokenCount ?? 0) / originalTokenCount) * 100)}%`);
    console.log(`
‚úÖ ÂúßÁ∏ÆÂá¶ÁêÜ„ÅåÂÆå‰∫Ü„Åó„Åæ„Åó„ÅüÔºÅ`);
    console.log(`üìâ „Éà„Éº„ÇØ„É≥Êï∞: ${originalTokenCount} ‚Üí ${newTokenCount} (${Math.round((1 - (newTokenCount ?? 0) / originalTokenCount) * 100)}%ÂâäÊ∏õ)`);

    return {
      originalTokenCount,
      newTokenCount: newTokenCount ?? 0,
    };
  }

  async handleQuotaError(
    authType?: string,
    error?: unknown,
  ): Promise<GeminiClient | null> {
    if (error) {
      console.warn('[Quota Error]:', getErrorMessage(error));
    }

    const flashModel = DEFAULT_AGENTS_FLASH_MODEL;
    const currentModel = this.config.getModel();
    
    if (currentModel === flashModel) {
      console.log('[Quota Error] Already on Flash model, cannot fallback further');
      return null;
    }

    console.log(`[Quota Error] Switching from ${currentModel} to ${flashModel} due to quota error`);
    
    // Update the config to use Flash model
    this.config.setModel(flashModel);
    
    // Return this client with updated model
    return this;
  }


}
